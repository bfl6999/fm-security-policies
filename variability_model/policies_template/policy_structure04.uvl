namespace Policies
imports
    k8s.Pod as Pod
    k8s.ServiceAccount as ServAcc
    k8s.RoleBinding as RoleBinding
    k8s.ClusterRoleBinding as ClusRole
    k8s.Service as Serv
    k8s.Ingress as Ingress
    k8s.Job as Job
    k8s.DaemonSet as DaemonSet
    k8s.Deployment as Deployment
    k8s.StatefulSet as StatefulSet
    k8s.Secret as Secret
    k8s.PersistentVolumeClaim as PersistVolumeClaim
    k8s.PodDisruptionBudget as PodDisrupBud
    k8s.CronJob as CronJob
    k8s.ReplicaSet as ReplicaSet
    k8s.ReplicationController as RepController
    k8s.Container as Container
    k8s.PodList as PodList
    k8s.PodTemplate as PodTemplate
    k8s.PodTemplateList as PodTemplateList
    k8s.PodTemplateSpec as PodTemplateSpec
    k8s.HorizontalPodAutoscaler as HorizontalPodAutoscaler
features
	PoliciesKubernetes {abstract}
		optional
			AWS_EKS_Best_Practices
				optional
					Require_aws_node_DaemonSet_use_IRSA {tool 'kyverno', severity 'medium', name 'require-aws-node-irsa', fields 'spec_template, spec_template_spec, spec_template_spec_serviceAccountName', kinds 'DaemonSet', doc 'According to EKS best practices, the aws-node DaemonSet is configured to use a role assigned to the EC2 instances to assign IPs to Pods This role includes several AWS managed policies that effectively allow all Pods running on a Node to attach/detach ENIs, assign/unassign IP addresses, or pull images from ECR Since this presents a risk to your cluster, it is recommended that you update the aws-node DaemonSet to use IRSA This policy ensures that the aws-node DaemonSet running in the kube-system Namespace is not still using the aws-node ServiceAccount', raw_source 'YAML'}
					Require_Encryption_with_AWS_LoadBalancers {tool 'kyverno', severity 'medium', name 'require-encryption-aws-loadbalancers', fields 'metadata_annotations, metadata_annotations_service_beta_kubernetes_io_aws_load_balancer_ssl_cert, spec_type', kinds 'Service', doc 'Services of type LoadBalancer when deployed inside AWS have support for transport encryption if it is enabled via an annotation This policy requires that Services of type LoadBalancer contain the annotation servicebetakubernetesio/aws-load-balancer-ssl-cert with some value', raw_source 'YAML'}
			Best_Practices
				optional
					Check_deprecated_APIs {tool 'kyverno', name 'check-deprecated-apis', kinds 'Kubernetes APIs', doc 'Kubernetes APIs are sometimes deprecated and removed after a few releases As a best practice, older API versions should be replaced with newer versions This policy validates for APIs that are deprecated or scheduled for removal Note that checking for some of these resources may require modifying the Kyverno ConfigMap to remove filters In the validate-v1-22-removals rule, the Lease kind has been commented out due to a check for this kind having a performance penalty on Kubernetes clusters with many leases Its enabling should be attended carefully and is not recommended on large clusters PodSecurityPolicy is removed in v125 so therefore the validate-v1-25-removals rule may not completely work on 125+ This policy requires Kyverno v174+ to function properly', raw_source 'YAML'}
					Disallow_empty_Ingress_host {tool 'kyverno', severity 'medium', name 'disallow-empty-ingress-host', kinds 'Ingress', doc 'An ingress resource needs to define an actual host name in order to be valid This policy ensures that there is a hostname for each rule defined', raw_source 'YAML'}
					Require_Labels {tool 'kyverno', severity 'medium', name 'require-labels', fields 'metadata_labels, metadata_labels_app_kubernetes_io_name', kinds 'Pod, Label', doc 'Define and use labels that identify semantic attributes of your application or Deployment A common set of labels allows tools to work collaboratively, describing objects in a common manner that all tools can understand The recommended labels describe applications in a way that can be queried This policy validates that the label appkubernetesio/name is specified with some value', raw_source 'YAML'}
					Disallow_NodePort {tool 'kyverno', severity 'medium', name 'restrict-nodeport', fields 'spec_type', kinds 'Service', doc 'A Kubernetes Service of type NodePort uses a host port to receive traffic from any source A NetworkPolicy cannot be used to control traffic to host ports Although NodePort Services can be useful, their use must be limited to Services with additional upstream security checks This policy validates that any new Services do not use the NodePort type', raw_source 'YAML'}
					Restrict_External_IPs {tool 'kyverno', severity 'medium', name 'restrict-external-ips', fields 'spec_XexternalIPs', kinds 'Service', doc 'Service externalIPs can be used for a MITM attack (CVE-2020-8554) Restrict externalIPs or limit to a known set of addresses See: https:_githubcom/kyverno/kyverno/issues/1367 This policy validates that the externalIPs field is not set on a Service', raw_source 'YAML'}
					Restrict_Ingress_defaultBackend {tool 'kyverno', severity 'high', name 'restrict-ingress-defaultbackend', fields 'spec_XdefaultBackend', kinds 'Ingress', doc 'An Ingress with no rules sends all traffic to a single default backend The defaultBackend is conventionally a configuration option of the Ingress controller and is not specified in your Ingress resources If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend In a multi-tenant environment, you want users to use explicit hosts, they should not be able to overwrite the global default backend service This policy prohibits the use of the defaultBackend field', raw_source 'YAML'}
			Best_Practices_EKS_Best_Practices
				optional
					Disallow_CRI_socket_mounts {tool 'kyverno', severity 'medium', name 'disallow-container-sock-mounts', fields 'spec_volumes', kinds 'Pod', doc 'Container daemon socket bind mounts allows access to the container engine on the node This access can be used for privilege escalation and to manage containers outside of Kubernetes, and hence should not be allowed This policy validates that the sockets used for CRI engines Docker, Containerd, and CRI-O are not used In addition to or replacement of this policy, preventing users from mounting the parent directories (/var/run and /var) may be necessary to completely prevent socket bind mounts', raw_source 'YAML'}
					Require_Limits_and_Requests {tool 'kyverno', severity 'medium', name 'require-requests-limits', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'As application workloads share cluster resources, it is important to limit resources requested and consumed by each Pod It is recommended to require resource requests and limits per Pod, especially for memory and CPU If a Namespace level request or limit is specified, defaults will automatically be applied to each Pod based on the LimitRange configuration This policy validates that all containers have something specified for memory and CPU requests and memory limits', raw_source 'YAML'}
					Restrict_Image_Registries {tool 'kyverno', severity 'medium', name 'restrict-image-registries', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Images from unknown, public registries can be of dubious quality and may not be scanned and secured, representing a high degree of risk Requiring use of known, approved registries helps reduce threat exposure by ensuring image pulls only come from them This policy validates that container images only originate from the registry eufooio or bario Use of this policy requires customization to define your allowable registries', raw_source 'YAML'}
			Multi_Tenancy
				optional
					Disallow_Default_Namespace {tool 'kyverno', severity 'medium', name 'disallow-default-namespace', fields 'metadata_namespace', kinds 'Pod', doc 'Kubernetes Namespaces are an optional feature that provide a way to segment and isolate cluster resources across multiple applications and users As a best practice, workloads should be isolated with Namespaces Namespaces should be required and the default (empty) Namespace should not be used This policy validates that Pods specify a Namespace name other than default Rule auto-generation is disabled here due to Pod controllers need to specify the namespace field under the top-level metadata object and not at the Pod template level', raw_source 'YAML'}
			Best_Practices_EKS_Best_Practices_PSP_Migration
				optional
					Require_Read_Only_Root_Filesystem {tool 'kyverno', severity 'medium', name 'require-ro-rootfs', fields 'spec_containers', kinds 'Pod', doc 'A read-only root file system helps to enforce an immutable infrastructure strategy; the container only needs to write on the mounted volume that persists the state An immutable root filesystem can also prevent malicious binaries from writing to the host system This policy validates that containers define a securityContext with readOnlyRootFilesystem: true', raw_source 'YAML'}
			Cert_Manager
				optional
					Certificate_max_duration_100_days {tool 'kyverno', severity 'medium', name 'cert-manager-limit-duration', kinds 'Certificate', doc 'Kubernetes managed non-letsencrypt certificates have to be renewed in every 100 days', raw_source 'YAML'}
					Restrict_issuer {tool 'kyverno', severity 'medium', name 'cert-manager-restrict-issuer', fields 'spec_dnsNames, spec_issuerRef, spec_issuerRef_group, spec_issuerRef_kind, spec_issuerRef_name', kinds 'Certificate', doc 'Certificates for trusted domains should always be steered to a controlled issuer to ensure the chain of trust is appropriate for that application Users may otherwise be able to create their own issuers and sign certificates for other domains This policy ensures that a certificate request for a specific domain uses a designated ClusterIssuer', raw_source 'YAML'}
			Consul
				optional
					Enforce_Consul_min_TLS_version {tool 'kyverno', severity 'medium', name 'enforce-min-tls-version', fields 'spec_tls, spec_tls_incoming, spec_tls_incoming_tlsMinVersion', kinds 'Mesh', doc 'This policy will check the TLS Min version to ensure that whenever the mesh is set, there is a minimum version of TLS set for all the service mesh proxies and this enforces that service mesh mTLS traffic uses TLS v12 or newer', raw_source 'YAML'}
			Flux
				optional
					Verify_Flux_Sources {tool 'kyverno', severity 'medium', name 'verify-flux-sources', fields 'spec_endpoint, spec_image, spec_url', kinds 'GitRepository, Bucket, HelmRepository, ImageRepository', doc 'Flux source APIs include a number of different sources such as GitRepository, Bucket, HelmRepository, and ImageRepository resources Each of these by default can be pointed to any location In a production environment, it may be desired to restrict these to only known sources to prevent accessing outside sources This policy verifies that each of the Flux sources comes from a trusted location', raw_source 'YAML'}
					Verify_Git_Repositories {tool 'kyverno', severity 'medium', name 'verify-git-repositories', fields 'spec_url', kinds 'GitRepository', doc 'Ensures that Git repositories used for Flux deployments in a cluster originate from a specific, trusted organization Prevents the use of untrusted or potentially risky Git repositories Protects the integrity and security of Flux deployments', raw_source 'YAML'}
			Istio
				optional
					Enforce_Istio_Ambient_Mode {tool 'kyverno', severity 'medium', name 'enforce-ambient-mode-namespace', fields 'metadata_labels, metadata_labels_istio_io_dataplane_mode', kinds 'Namespace', doc 'In order for Istio to include namespaces in ambient mode, the label istioio/dataplane-mode must be set to ambient This policy ensures that all new Namespaces set istioio/dataplane-mode to ambient', raw_source 'YAML'}
					Enforce_Istio_Sidecar_Injection {tool 'kyverno', severity 'medium', name 'enforce-sidecar-injection-namespace', fields 'metadata_labels, metadata_labels_istio_injection', kinds 'Namespace', doc 'In order for Istio to inject sidecars to workloads deployed into Namespaces, the label istio-injection must be set to enabled This policy ensures that all new Namespaces set istio-inject to enabled', raw_source 'YAML'}
					Enforce_Istio_Strict_mTLS {tool 'kyverno', severity 'medium', name 'enforce-strict-mtls', fields 'spec_mtls, spec_mtls_mode', kinds 'PeerAuthentication', doc 'Strict mTLS requires that mutual TLS be enabled across the entire service mesh, which can be set using a PeerAuthentication resource on a per-Namespace basis and, if set on the istio-system Namespace could disable it across the entire mesh Disabling mTLS can reduce the security for traffic within that portion of the mesh and should be controlled This policy prevents disabling strict mTLS in a PeerAuthentication resource by requiring the mode be set to either UNSET or STRICT', raw_source 'YAML'}
					Enforce_Istio_TLS_on_Hosts_and_Host_Subnets {tool 'kyverno', severity 'medium', name 'enforce-tls-hosts-host-subnets', fields 'spec_trafficPolicy, spec_trafficPolicy_tls, spec_trafficPolicy_tls_mode', kinds 'DestinationRule', doc 'Once a routing decision has been made, a DestinationRule can be used to define how traffic should be sent to another service The trafficPolicy object can control how TLS is handled to the destination host This policy enforces that the TLS mode cannot be set to a value of DISABLE', raw_source 'YAML'}
					Prevent_Disabling_Istio_Sidecar_Injection {tool 'kyverno', severity 'medium', name 'prevent-disabling-injection-pods', fields 'metadata_annotations, metadata_annotations_sidecar_istio_io_inject', kinds 'Pod', doc 'One way sidecar injection in an Istio service mesh may be accomplished is by defining an annotation at the Pod level Pods not receiving a sidecar cannot participate in the mesh thereby reducing visibility This policy ensures that Pods cannot set the annotation sidecaristioio/inject to a value of false', raw_source 'YAML'}
			Veeam_Kasten
				optional
					Check_Kasten_3_2_1_Backup_Policy {tool 'kyverno', severity 'medium', name 'kasten-3-2-1-backup-policy', kinds 'Policy', doc 'The 3-2-1 rule of data protection recommends that you have at least 3 copies of data, on 2 different storage targets, with 1 being offsite This approach ensures a health mix of redundancy options for data recovery of the application for localized & multi-region cloud failures or compromise In Kubernetes, this translates to the original running resources, a local snapshot, and a copy of all application resources and volume data exported to an external repository This policy accomplishes 3-2-1 validation by ensuring each policy contains both _action: backup_ and _action: export_', raw_source 'YAML'}
					Check_Data_Protection_By_Label {tool 'kyverno', name 'kasten-data-protection-by-label', fields 'metadata_labels, metadata_labels_dataprotection', kinds 'Deployment, StatefulSet', doc 'Check the _dataprotection_ label for production Deployments and StatefulSet workloads Use in combination with _kasten-generate-example-backup-policy_ policy to generate a Kasten policy for the workload namespace, if it doesn_t already exist', raw_source 'YAML'}
					Check_Kasten_Location_Profile_is_Immutable {tool 'kyverno', name 'kasten-immutable-location-profile', fields 'spec_locationSpec, spec_locationSpec_objectStore, spec_locationSpec_objectStore_protectionPeriod, spec_type', kinds 'config.kio.kasten.io/v1alpha1/Profile', doc 'Ensure Kasten Location Profiles have enabled immutability to prevent unintentional or malicious changes to backup data', raw_source 'YAML'}
					Validate_Data_Protection_with_Kasten_Preset_Label {tool 'kyverno', name 'kasten-validate-ns-by-preset-label', fields 'metadata_labels, metadata_labels_dataprotection', kinds 'Namespace', doc 'Kubernetes applications are typically deployed into a single, logical namespace  Veeam Kasten policies will discover and protect all resources within the selected namespace(s)  This policy ensures all new namespaces include a label referencing a valid Kasten SLA  (Policy Preset) for data protectionThis policy can be used in combination with /Users/the kasten-generate-policy-by-preset-label ClusterPolicy to automatically create a Kasten policy based on the specified SLA  The combination ensures that new applications are not inadvertently left unprotected', raw_source 'YAML'}
			Kubecost
				optional
					Require_Kubecost_Labels {tool 'kyverno', severity 'medium', name 'require-kubecost-labels', fields 'metadata_labels, metadata_labels_app, metadata_labels_department, metadata_labels_env, metadata_labels_owner, metadata_labels_team', kinds 'Pod, Label', doc 'Kubecost can use labels assigned to Pods in order to track and display cost allocation in a granular way These labels, which can be customized, can be used to organize and group workloads in different ways This policy requires that the labels owner, team, department, app, and env are all defined on Pods With Kyverno autogen enabled (absence of the annotation pod-policieskyvernoio/autogen-controllers=none), these labels will also be required for all Pod controllers', raw_source 'YAML'}
			KubeVirt
				optional
					Enforce_instanceTypes {tool 'kyverno', name 'k6t-enforce-instancetype', fields 'spec_instancetype, spec_instancetype_name, spec_preference, spec_preference_name', kinds 'VirtualMachine', doc 'Check VirtualMachines and validate that they are using an instance type and preference', raw_source 'YAML'}
			Linkerd
				optional
					Prevent_Linkerd_Pod_Injection_Override {tool 'kyverno', severity 'medium', name 'prevent-linkerd-pod-injection-override', fields 'metadata_annotations, metadata_annotations_linkerd_io_inject', kinds 'Pod', doc 'Setting the annotation on a Pod (or its controller) linkerdio/inject to disabled may effectively disable mesh participation for that workload reducing security and visibility This policy prevents setting the annotation linkerdio/inject to disabled for Pods', raw_source 'YAML'}
					Prevent_Linkerd_Port_Skipping {tool 'kyverno', severity 'medium', name 'prevent-linkerd-port-skipping', fields 'metadata_annotations, metadata_annotations_Xconfig_linkerd_io_skip_inbound_ports, metadata_annotations_Xconfig_linkerd_io_skip_outbound_ports', kinds 'Pod', doc 'Linkerd has the ability to skip inbound and outbound ports assigned to Pods, exempting them from mTLS This can be important in some narrow use cases but generally should be avoided This policy prevents Pods from setting the annotations configlinkerdio/skip-inbound-ports or configlinkerdio/skip-outbound-ports', raw_source 'YAML'}
					Require_Linkerd_Mesh_Injection {tool 'kyverno', severity 'medium', name 'require-linkerd-mesh-injection', fields 'metadata_annotations, metadata_annotations_linkerd_io_inject', kinds 'Namespace, Annotation', doc 'Sidecar proxy injection in Linkerd may be handled at the Namespace level by setting the annotation linkerdio/inject to enabled This policy enforces that all Namespaces contain the annotation linkerdio/inject set to enabled', raw_source 'YAML'}
			Security_NGINX_Ingress
				optional
					Disallow_Custom_Snippets {tool 'kyverno', name 'disallow-ingress-nginx-custom-snippets', fields 'data_allow_snippet_annotations, metadata_annotations, metadata_annotations_X*_snippet', kinds 'ConfigMap, Ingress', doc 'Users that can create or update ingress objects can use the custom snippets  feature to obtain all secrets in the cluster (CVE-2021-25742) This policy  disables allow-snippet-annotations in the ingress-nginx configuration and  blocks *-snippet annotations on an Ingress See: https:_githubcom/kubernetes/ingress-nginx/issues/7837', raw_source 'YAML'}
					Restrict_NGINX_Ingress_path_values {tool 'kyverno', severity 'high', name 'restrict-ingress-paths', kinds 'Ingress', doc 'This policy mitigates CVE-2021-25745 by restricting specrules[]httppaths[]path to safe values Additional paths can be added as required This issue has been fixed in NGINX Ingress v120  Please refer to the CVE for details', raw_source 'YAML'}
			OpenShift
				optional
					Disallow_deprecated_APIs {tool 'kyverno', severity 'medium', name 'disallow-deprecated-apis', kinds 'ClusterRole,ClusterRoleBinding,Role,RoleBinding,RBAC', doc 'OpenShift APIs are sometimes deprecated and removed after a few releases As a best practice, older API versions should be replaced with newer versions This policy validates for APIs that are deprecated or scheduled for removal Note that checking for some of these resources may require modifying the Kyverno ConfigMap to remove filters      ', raw_source 'YAML'}
			Other
				optional
					Allowed_Annotations {tool 'kyverno', severity 'medium', name 'allowed-annotations', kinds 'Pod, Annotation', doc 'Rather than creating a deny list of annotations, it may be more useful to invert that list and create an allow list which then denies any others This policy demonstrates how to allow two annotations with a specific key name of fluxcdio/ while denying others that do not meet the pattern', raw_source 'YAML'}
					Allowed_Label_Changes {tool 'kyverno', severity 'medium', name 'allowed-label-changes', kinds 'Pod,Label', doc 'In some cases, operations teams need a type of limited access to change resources during troubleshooting or outage mitigation This policy demonstrates how to prevent modification to labels except one with the key breakglass Changing, adding, or deleting any other labels is denied', raw_source 'YAML'}
					Block_Ephemeral_Containers {tool 'kyverno', severity 'medium', name 'block-ephemeral-containers', fields 'spec_XephemeralContainers', kinds 'Pod', doc 'Ephemeral containers, enabled by default in Kubernetes 123, allow users to use the kubectl debug functionality and attach a temporary container to an existing Pod This may potentially be used to gain access to unauthorized information executing inside one or more containers in that Pod This policy blocks the use of ephemeral containers', raw_source 'YAML'}
					Check_Environment_Variables {tool 'kyverno', severity 'medium', name 'check-env-vars', fields 'spec_containers', kinds 'Pod', doc 'Environment variables control many aspects of a container_s execution and are often the source of many different configuration settings Being able to ensure that the value of a specific environment variable either is or is not set to a specific string is useful to maintain such controls This policy checks every container to ensure that if the DISABLE_OPA environment variable is defined, it must not be set to a value of true', raw_source 'YAML'}
					Deny_Commands_in_Exec_Probe {tool 'kyverno', name 'deny-commands-in-exec-probe', kinds 'Pod', doc 'Developers may feel compelled to use simple shell commands as a workaround to creating proper liveness or readiness probes for a Pod Such a practice can be discouraged via detection of those commands This policy prevents the use of certain commands jcmd, ps, or ls if found in a Pod_s liveness exec probe', raw_source 'YAML'}
					Disallow_all_Secrets {tool 'kyverno', severity 'medium', name 'no-secrets', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers, spec_volumes', kinds 'Pod, Secret', doc 'Secrets often contain sensitive information which not all Pods need consume This policy disables the use of all Secrets in a Pod definition In order to work effectively, this Policy needs a separate Policy or rule to require automountServiceAccountToken=false at the Pod level or ServiceAccount level since this would otherwise result in a Secret being mounted', raw_source 'YAML'}
					Docker_Socket_Requires_Label {tool 'kyverno', severity 'medium', name 'docker-socket-check', fields 'metadata_labels, metadata_labels_allow_docker, spec_volumes', kinds 'Pod', doc 'Accessing a container engine_s socket is for highly specialized use cases and should generally be disabled If access must be granted, it should be done on an explicit basis This policy requires that, for any Pod mounting the Docker socket, it must have the label allow-docker set to true', raw_source 'YAML'}
					Forbid_CPU_Limits {tool 'kyverno', name 'forbid-cpu-limits', fields 'spec_containers', kinds 'Pod', doc 'Setting of CPU limits is a debatable poor practice as it can result, when defined, in potentially starving applications of much-needed CPU cycles even when they are available Ensuring that CPU limits are not set may ensure apps run more effectively This policy forbids any container in a Pod from defining CPU limits', raw_source 'YAML'}
					Inspect_CertificateSigningRequest {tool 'kyverno', name 'inspect-csr', kinds 'CertificateSigningRequest', doc 'The Kubernetes API includes a CertificateSigningRequest resource which can be used to generate a certificate for an entity Because this API can be abused to create a long-lived credential, it is important to be able to audit this API to understand who/what is creating these CSRs and for what actors they are being created This policy, intended to always be run in Audit mode and produce failure results in a Policy Report, inspects all incoming CertificateSigningRequests and writes out into the Policy Report information on who/what requested it and parsing the CSR to show the Subject information of that CSR resource', raw_source 'YAML'}
					Namespace_Protection {tool 'kyverno', severity 'medium', name 'namespace-protection', kinds 'Namespace', doc 'Cases where RBAC may be applied at a higher level and where Namespace-level protections may be necessary can be accomplished with a separate policy For example, one may want to protect creates, updates, and deletes on only a single Namespace This policy will block creates, updates, and deletes to any Namespace labeled with freeze=true Caution should be exercised when using rules which match on all kinds (*) as this will involve, for larger clusters, a substantial amount of processing on Kyverno_s part Additional resource requests and/or limits may be required', raw_source 'YAML'}
					PodDisruptionBudget_maxUnavailable_Non_Zero {tool 'kyverno', name 'pdb-maxunavailable', fields 'spec_maxUnavailable', kinds 'PodDisruptionBudget', doc 'A PodDisruptionBudget which sets its maxUnavailable value to zero prevents all voluntary evictions including Node drains which may impact maintenance tasks This policy enforces that if a PodDisruptionBudget specifies the maxUnavailable field it must be greater than zero', raw_source 'YAML'}
					Prevent_cr8escape_CVE_2022_0811 {tool 'kyverno', severity 'high', name 'prevent-cr8escape', fields 'spec_securityContext, spec_securityContext_sysctls', kinds 'Pod', doc 'A vulnerability cr8escape (CVE-2022-0811) in CRI-O the container runtime engine underpinning Kubernetes allows attackers to escape from a Kubernetes container and gain root access to the host The recommended remediation is to disallow sysctl settings with + or = in their value', raw_source 'YAML'}
					Prevent_Duplicate_HorizontalPodAutoscalers {tool 'kyverno', severity 'medium', name 'prevent-duplicate-hpa', fields 'spec_scaleTargetRef, spec_scaleTargetRef_kind', kinds 'HorizontalPodAutoscaler', doc 'HorizontalPodAutoscaler (HPA) is useful to automatically adjust the number of pods in a deployment or replication controller It requires defining a specific target resource by kind and name There are no built-in validation checks by the HPA controller to prevent the creation of multiple HPAs which target the same resource This policy has two rules, the first of which ensures that the only targetRef kinds accepted are one of either Deployment, StatefulSet, ReplicaSet, or DaemonSet The second prevents the creation of duplicate HPAs by validating that any new HPA targets a unique resource', raw_source 'YAML'}
					Prevent_Duplicate_VerticalPodAutoscalers {tool 'kyverno', severity 'medium', name 'prevent-duplicate-vpa', fields 'spec_targetRef, spec_targetRef_kind', kinds 'VerticalPodAutoscaler', doc 'VerticalPodAutoscaler (VPA) is useful to automatically adjust the resources assigned to Pods It requires defining a specific target resource by kind and name There are no built-in validation checks by the VPA controller to prevent the creation of multiple VPAs which target the same resource This policy has two rules, the first of which ensures that the only targetRef kinds accepted are one of either Deployment, StatefulSet, ReplicaSet, or DaemonSet The second prevents the creation of duplicate VPAs by validating that any new VPA targets a unique resource', raw_source 'YAML'}
					Require_Annotations {tool 'kyverno', severity 'medium', name 'require-annotations', fields 'metadata_annotations, metadata_annotations_corp_org_department', kinds 'Pod, Annotation', doc 'Define and use annotations that identify semantic attributes of your application or Deployment A common set of annotations allows tools to work collaboratively, describing objects in a common manner that all tools can understand The recommended annotations describe applications in a way that can be queried This policy validates that the annotation corporg/department is specified with some value      ', raw_source 'YAML'}
					Require_Container_Port_Names {tool 'kyverno', severity 'medium', name 'require-container-port-names', fields 'spec_containers', kinds 'Pod', doc 'Containers may define ports on which they listen In addition to a port number, a name field may optionally be used Including a name makes it easier when defining Service resource definitions and others since the name may be referenced allowing the port number to change This policy requires that for every containerPort defined there is also a name specified      ', raw_source 'YAML'}
					Require_CPU_Limits {tool 'kyverno', severity 'medium', name 'require-cpu-limits', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Setting CPU limits on containers ensures fair distribution of resources, preventing any single container from monopolizing CPU and impacting the performance of other containers This practice enhances stability, predictability, and cost control, while also mitigating the noisy neighbor problem and facilitating efficient scaling in Kubernetes clusters This policy ensures that cpu limits are set on every container', raw_source 'YAML'}
					Require_Reasonable_PodDisruptionBudgets {tool 'kyverno', name 'require-reasonable-pdbs', kinds 'PodDisruptionBudget', doc 'PodDisruptionBudget resources are useful to ensuring minimum availability is maintained at all times Achieving a balance between availability and maintainability is important This policy validates that a PodDisruptionBudget, specified as percentages, allows 50% of the replicas to be out of service in that minAvailable should be no higher than 50% and maxUnavailable should be no lower than 50%', raw_source 'YAML'}
					Restrict_Jobs {tool 'kyverno', severity 'medium', name 'restrict-jobs', kinds 'Job', doc 'Jobs can be created directly and indirectly via a CronJob controller In some cases, users may want to only allow Jobs if they are created via a CronJob This policy restricts Jobs so they may only be created by a CronJob', raw_source 'YAML'}
					Restrict_Node_Affinity {tool 'kyverno', severity 'medium', name 'restrict-node-affinity', fields 'spec_affinity, spec_affinity_XnodeAffinity', kinds 'Pod', doc 'Pods may use several mechanisms to prefer scheduling on a set of nodes, and nodeAffinity is one of them nodeAffinity uses expressions to select eligible nodes for scheduling decisions and may override intended placement options by cluster administrators This policy ensures that nodeAffinity is not used in a Pod spec', raw_source 'YAML'}
					Restrict_Pod_Controller_ServiceAccount_Updates {tool 'kyverno', severity 'Medium', name 'restrict-pod-controller-serviceaccount-updates', kinds 'Pod', doc 'ServiceAccounts which have the ability to edit/patch workloads which they created may potentially use that privilege to update to a different ServiceAccount with higher privileges This policy, intended to be run in enforce mode, blocks updates to Pod controllers if those updates modify the serviceAccountName field Updates to Pods directly for this field are not possible as it is immutable once set', raw_source 'YAML'}
					Restrict_Scale {tool 'kyverno', severity 'medium', name 'restrict-scale', fields 'spec_replicas, status_selector', kinds 'Deployment', doc 'Pod controllers such as Deployments which implement replicas and permit the scale action use a /scale subresource to control this behavior In addition to checks for creations of such controllers that their replica is in a certain shape, the scale operation and subresource needs to be accounted for as well This policy, operable beginning in Kyverno 19, is a collection of rules which can be used to limit the replica count both upon creation of a Deployment and when a scale operation is performed', raw_source 'YAML'}
					Restrict_Service_Port_Range {tool 'kyverno', severity 'medium', name 'restrict-service-port-range', fields 'spec_ports', kinds 'Service', doc 'Services which are allowed to expose any port number may be able to impact other applications running on the Node which require them, or may make specifying security policy externally more challenging This policy enforces that only the port range 32000 to 33000 may be used for Service resources', raw_source 'YAML'}
					Verify_VerticalPodAutoscaler_Target {tool 'kyverno', severity 'medium', name 'verify-vpa-target', fields 'spec_targetRef, spec_targetRef_kind', kinds 'VerticalPodAutoscaler', doc 'VerticalPodAutoscaler (VPA) is useful to automatically adjust the resources assigned to Pods It requires defining a specific target resource by kind and name There are no built-in validation checks by the VPA controller to ensure that the target resource exists or that the target kind is specified correctly This policy contains two rules, the first of which verifies that the kind is specified exactly as Deployment, StatefulSet, ReplicaSet, or DaemonSet, which helps avoid typos The second rule verifies that the target resource exists before allowing the VPA to be created', raw_source 'YAML'}
			Sample
				optional
					Block_kubectl_cp_command_by_Pod_Label {tool 'kyverno', name 'block-kubectl-cp-by-pod-label', kinds 'Pod', doc 'The kubectl cp command is used to copy files between a local machine and a Pod_s container  While this functionality is useful for transferring data, it may introduce security risks,  such as unauthorized data exfiltration or modification This policy blocks the use of the  kubectl cp command on all Pods with label block-kubectl-cp=true, ensuring that sensitive  workloads are protected from unintended file transfers Other kubectl operations are unaffected,  allowing for normal Pod management while preventing potential misuse of file copy capabilities', raw_source 'YAML'}
					Block_Pod_Exec_by_Pod_and_Container {tool 'kyverno', name 'deny-exec-by-pod-and-container', kinds 'Pod', doc 'The exec command may be used to gain shell access, or run other commands, in a Pod_s container While this can be useful for troubleshooting purposes, it could represent an attack vector and is discouraged This policy blocks Pod exec commands to containers named nginx in Pods starting with name myapp-maintenance', raw_source 'YAML'}
					Disallow_Localhost_ExternalName_Services {tool 'kyverno', severity 'medium', name 'no-localhost-service', fields 'spec_externalName, spec_type', kinds 'Service', doc 'A Service of type ExternalName which points back to localhost can potentially be used to exploit vulnerabilities in some Ingress controllers This policy audits Services of type ExternalName if the externalName field refers to localhost', raw_source 'YAML'}
					Enforce_pod_duration {tool 'kyverno', name 'pod-lifetime', kinds 'Pod', doc 'This validation is valuable when annotations are used to define durations, such as to ensure a Pod lifetime annotation does not exceed some site specific max threshold Pod lifetime annotation can be no greater than 8 hours', raw_source 'YAML'}
					Enforce_ReadWriteOncePod {tool 'kyverno', name 'readwriteonce-pod', fields 'spec_accessModes', kinds 'PersistentVolumeClaim', doc 'Some stateful workloads with multiple replicas only allow a single Pod to write to a given volume at a time Beginning in Kubernetes 122 and enabled by default in 127, a new setting called ReadWriteOncePod, available for CSI volumes only, allows volumes to be writable from only a single Pod For more information see the blog https:_kubernetesio/blog/2023/04/20/read-write-once-pod-access-mode-beta/ This policy enforces that the accessModes for a PersistentVolumeClaim be set to ReadWriteOncePod', raw_source 'YAML'}
					Validate_Probes {tool 'kyverno', severity 'medium', name 'validate-probes', kinds 'Pod', doc 'Liveness and readiness probes accomplish different goals, and setting both to the same is an anti-pattern and often results in app problems in the future This policy checks that liveness and readiness probes are not equal Keep in mind that if both the  probes are not set, they are considered to be equal and hence fails the check', raw_source 'YAML'}
					Require_imagePullPolicy_Always {tool 'kyverno', severity 'medium', name 'imagepullpolicy-always', fields 'spec_containers', kinds 'Pod', doc 'If the latest tag is allowed for images, it is a good idea to have the imagePullPolicy field set to Always to ensure should that tag be overwritten that future pulls will get the updated image This policy validates the imagePullPolicy is set to Always when the latest tag is specified explicitly or where a tag is not defined at all', raw_source 'YAML'}
					Memory_Requests_Equal_Limits {tool 'kyverno', severity 'medium', name 'memory-requests-equal-limits', kinds 'Pod', doc 'Pods which have memory limits equal to requests could be given a QoS class of Guaranteed if they also set CPU limits equal to requests Guaranteed is the highest schedulable class  This policy checks that all containers in a given Pod have memory requests equal to limits', raw_source 'YAML'}
					Require_Multiple_Replicas {tool 'kyverno', severity 'medium', name 'deployment-has-multiple-replicas', fields 'spec_replicas', kinds 'Deployment', doc 'Deployments with a single replica cannot be highly available and thus the application may suffer downtime if that one replica goes down This policy validates that Deployments have more than one replica', raw_source 'YAML'}
					Require_Images_Use_Checksums {tool 'kyverno', severity 'medium', name 'require-image-checksum', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Use of a SHA checksum when pulling an image is often preferable because tags are mutable and can be overwritten This policy checks to ensure that all images use SHA checksums rather than tags', raw_source 'YAML'}
					Restrict_Annotations {tool 'kyverno', name 'restrict-annotations', fields 'metadata_annotations, metadata_annotations_Xfluxcd_io_*', kinds 'Pod, Annotation', doc 'Some annotations control functionality driven by other cluster-wide tools and are not normally set by some class of users This policy prevents the use of an annotation beginning with fluxcdio/ This can be useful to ensure users either don_t set reserved annotations or to force them to use a newer version of an annotation', raw_source 'YAML'}
					Restrict_ClusterRole_with_Nodes_Proxy {tool 'kyverno', severity 'medium', name 'restrict-clusterrole-nodesproxy', kinds 'ClusterRole, RBAC', doc 'A ClusterRole with nodes/proxy resource access allows a user to perform anything the kubelet API allows It also allows users to bypass the API server and talk directly to the kubelet potentially circumventing audits and admission controllers See https:_blogaquaseccom/privilege-escalation-kubernetes-rbac for more info This policy prevents the creation of a ClusterRole if it contains the nodes/proxy resource ', raw_source 'YAML'}
					Restrict_control_plane_scheduling {tool 'kyverno', name 'restrict-controlplane-scheduling', fields 'spec_tolerations', kinds 'Pod', doc 'Scheduling non-system Pods to control plane nodes (which run kubelet) is often undesirable because it takes away resources from the control plane components and can represent a possible security threat vector This policy prevents users from setting a toleration in a Pod spec which allows running on control plane nodes with the taint key node-rolekubernetesio/master', raw_source 'YAML'}
					Restrict_Ingress_Classes {tool 'kyverno', severity 'medium', name 'restrict-ingress-classes', fields 'metadata_annotations, metadata_annotations_kubernetes_io_ingress_class', kinds 'Ingress', doc 'Ingress classes should only be allowed which match up to deployed Ingress controllers in the cluster Allowing users to define classes which cannot be satisfied by a deployed Ingress controller can result in either no or undesired functionality This policy checks Ingress resources and only allows those which define HAProxy or nginx in the respective annotation This annotation has largely been replaced as of Kubernetes 118 with the IngressClass resource', raw_source 'YAML'}
					Disallow_Service_Type_LoadBalancer {tool 'kyverno', severity 'medium', name 'no-loadbalancer-service', fields 'spec_type', kinds 'Service', doc 'Especially in cloud provider environments, a Service having type LoadBalancer will cause the provider to respond by creating a load balancer somewhere in the customer account This adds cost and complexity to a deployment Without restricting this ability, users may easily overrun established budgets and security practices set by the organization This policy restricts use of the Service type LoadBalancer', raw_source 'YAML'}
					Restrict_node_label_changes {tool 'kyverno', name 'protect-node-label-foo', fields 'metadata_labels, metadata_labels_foo', kinds 'Node, Label', doc 'Node labels are critical pieces of metadata upon which many other applications and logic may depend and should not be altered or removed by regular users This policy prevents changes or deletions to a label called foo on cluster Nodes Use of this policy requires removal of the Node resource filter in the Kyverno ConfigMap ([Node,*,*]) Due to Kubernetes CVE-2021-25735, this policy requires, at minimum, one of the following versions of Kubernetes: v11818, v11910, v1206, or v1210', raw_source 'YAML'}
					Validate_User_ID_Group_ID_and_FS_Group {tool 'kyverno', severity 'medium', name 'validate-userid-groupid-fsgroup', fields 'spec_securityContext, spec_securityContext_fsGroup, spec_securityContext_runAsGroup, spec_securityContext_runAsUser', kinds 'Pod', doc 'All processes inside a Pod can be made to run with specific user and groupID by setting runAsUser and runAsGroup respectively fsGroup can be specified to make sure any file created in the volume will have the specified groupID This policy validates that these fields are set to the defined values', raw_source 'YAML'}
					Spread_Pods_Across_Nodes_&_Zones {tool 'kyverno', severity 'medium', name 'topologyspreadconstraints-policy', kinds 'Deployment, StatefulSet', doc 'Deployments to a Kubernetes cluster with multiple availability zones often need to distribute those replicas to align with those zones to ensure site-level failures do not impact availability This policy ensures topologySpreadConstraints are defined,  to spread pods over nodes and zones Deployments or Statefulsets with leass than 3  replicas are skipped', raw_source 'YAML'}
			Ingress_Security
				optional
					Ensure_Valid_Ingress_NGINX_Controller_and_Annotations {tool 'kyverno', severity 'high', name 'check-ingress-nginx-controller-version-and-annotation-policy', fields 'metadata_annotations, metadata_annotations_Xnginx_ingress_kubernetes_io_server_snippet, spec_containers', kinds 'Ingress, Pod', doc 'This policy ensures that Ingress resources do not have certain disallowed annotations and that the ingress-nginx controller Pod is running an appropriate version of the image It checks for the presence of the  nginxingresskubernetesio/server-snippet annotation and disallows its usage, enforces specific values  for auth-tls-verify-client, and ensures that the ingress-nginx controller image is of the required version', raw_source 'YAML'}
			Security
				optional
					Check_Long_Lived_Secrets_in_ServiceAccounts {tool 'kyverno', severity 'medium', name 'check-serviceaccount-secrets', kinds 'Secret,ServiceAccount', doc 'Before version 124, Kubernetes automatically generated Secret-based tokens  for ServiceAccounts To distinguish between automatically generated tokens  and manually created ones, Kubernetes checks for a reference from the  ServiceAccount_s secrets field If the Secret is referenced in the secrets  field, it is considered an auto-generated legacy token These legacy Tokens can be of security concern and should be audited', raw_source 'YAML'}
					Deny_Secret_Service_Account_Token_Type {tool 'kyverno', severity 'medium', name 'deny-secret-service-account-token-type', kinds 'Secret, ServiceAccount', doc 'Before version 124, Kubernetes automatically generated Secret-based tokens  for ServiceAccounts When creating a Secret, you can specify its type using the  type field of the Secret resource  The type kubernetesio/service-account-token is used for legacy ServiceAccount tokens  These legacy Tokens can be of security concern and should be audited', raw_source 'YAML'}
					Restrict_Binding_to_Cluster_Admin {tool 'kyverno', severity 'medium', name 'restrict-binding-clusteradmin', fields 'roleRef_name', kinds 'RoleBinding, ClusterRoleBinding, RBAC', doc 'The cluster-admin ClusterRole allows any action to be performed on any resource in the cluster and its granting should be heavily restricted This policy prevents binding to the cluster-admin ClusterRole in RoleBinding or ClusterRoleBinding resources', raw_source 'YAML'}
					Restrict_Edit_for_Endpoints_CVE_2021_25740 {tool 'kyverno', severity 'low', name 'restrict-edit-for-endpoints', kinds 'ClusterRole', doc 'Clusters not initially installed with Kubernetes 122 may be vulnerable to an issue defined in CVE-2021-25740 which could enable users to send network traffic to locations they would otherwise not have access to via a confused deputy attack This was due to the system:aggregate-to-edit ClusterRole having edit permission of Endpoints This policy, intended to run in background mode, checks if your cluster is vulnerable to CVE-2021-25740 by ensuring the system:aggregate-to-edit ClusterRole does not have the edit permission of Endpoints', raw_source 'YAML'}
					Restrict_Auto_Mount_of_Service_Account_Tokens_in_Service_Account {tool 'kyverno', severity 'medium', name 'restrict-sa-automount-sa-token', kinds 'Secret,ServiceAccount', doc 'Kubernetes automatically mounts ServiceAccount credentials in each ServiceAccount The ServiceAccount may be assigned roles allowing Pods to access API resources Blocking this ability is an extension of the least privilege best practice and should be followed if Pods do not need to speak to the API server to function This policy ensures that mounting of these ServiceAccount tokens is blocked      ', raw_source 'YAML'}
					Restrict_Secret_Verbs_in_Roles {tool 'kyverno', severity 'medium', name 'restrict-secret-role-verbs', kinds 'Role, ClusterRole, RBAC', doc 'The verbs get, list, and watch in a Role or ClusterRole, when paired with the Secrets resource, effectively allows Secrets to be read which may expose sensitive information This policy prevents a Role or ClusterRole from using these verbs in tandem with Secret resources In order to fully implement this control, it is recommended to pair this policy with another which also prevents use of the wildcard (_*_) in the verbs list either when explicitly naming Secrets or when also using a wildcard in the base API group', raw_source 'YAML'}
			Sample_EKS_Best_Practices
				optional
					Disallow_Secrets_from_Env_Vars {tool 'kyverno', severity 'medium', name 'secrets-not-from-env-vars', fields 'spec_containers', kinds 'Pod, Secret', doc 'Secrets used as environment variables containing sensitive information may, if not carefully controlled,  be printed in log output which could be visible to unauthorized people and captured in forwarding applications This policy disallows using Secrets as environment variables', raw_source 'YAML'}
			Multi_Tenancy_EKS_Best_Practices
				optional
					Require_Pod_priorityClassName {tool 'kyverno', severity 'medium', name 'require-pod-priorityclassname', fields 'spec_priorityClassName', kinds 'Pod', doc 'A Pod may optionally specify a priorityClassName which indicates the scheduling priority relative to others This requires creation of a PriorityClass object in advance With this created, a Pod may set this field to that value In a multi-tenant environment, it is often desired to require this priorityClassName be set to make certain tenant scheduling guarantees This policy requires that a Pod defines the priorityClassName field with some value', raw_source 'YAML'}
			Other_Multi_Tenancy
				optional
					Require_QoS_Burstable {tool 'kyverno', severity 'medium', name 'require-qos-burstable', kinds 'Pod', doc 'Pod Quality of Service (QoS) is a mechanism to ensure Pods receive certain priority guarantees based upon the resources they define When a Pod has at least one container which defines either requests or limits for either memory or CPU, Kubernetes grants the QoS class as burstable if it does not otherwise qualify for a QoS class of guaranteed This policy requires that a Pod meet the criteria qualify for a QoS of burstable This policy is provided with the intention that users will need to control its scope by using exclusions, preconditions, and other policy language mechanisms', raw_source 'YAML'}
					Require_StorageClass {tool 'kyverno', severity 'medium', name 'require-storageclass', fields 'spec_storageClassName, spec_volumeClaimTemplates', kinds 'PersistentVolumeClaim, StatefulSet', doc 'PersistentVolumeClaims (PVCs) and StatefulSets may optionally define a StorageClass to dynamically provision storage In a multi-tenancy environment where StorageClasses are far more common, it is often better to require storage only be provisioned from these StorageClasses This policy requires that PVCs and StatefulSets containing volumeClaimTemplates define the storageClassName field with some value', raw_source 'YAML'}
					Restrict_StorageClass {tool 'kyverno', severity 'medium', name 'restrict-storageclass', kinds 'StorageClass', doc 'StorageClasses allow description of custom classes of storage offered by the cluster, based on quality-of-service levels, backup policies, or custom policies determined by the cluster administrators For shared StorageClasses in a multi-tenancy environment, a reclaimPolicy of Delete should be used to ensure a PersistentVolume cannot be reused across Namespaces This policy requires StorageClasses set a reclaimPolicy of Delete', raw_source 'YAML'}
			Security_EKS_Best_Practices
				optional
					Restrict_Binding_System_Groups {tool 'kyverno', severity 'medium', name 'restrict-binding-system-groups', kinds 'RoleBinding, ClusterRoleBinding, RBAC', doc 'Certain system groups exist in Kubernetes which grant permissions that are used for certain system-level functions yet typically never appropriate for other users This policy prevents creating bindings to some of these groups including system:anonymous, system:unauthenticated, and system:masters', raw_source 'YAML'}
			Pod_Security_Standards_Baseline
				optional
					Disallow_Capabilities {tool 'kyverno', severity 'medium', name 'disallow-capabilities', kinds 'Pod', doc 'Adding capabilities beyond those listed in the policy must be disallowed', raw_source 'YAML'}
					Disallow_Host_Namespaces {tool 'kyverno', severity 'medium', name 'disallow-host-namespaces', fields 'spec_hostIPC, spec_hostNetwork, spec_hostPID', kinds 'Pod', doc 'Host namespaces (Process ID namespace, Inter-Process Communication namespace, and network namespace) allow access to shared information and can be used to elevate privileges Pods should not be allowed access to host namespaces This policy ensures fields which make use of these host namespaces are unset or set to false', raw_source 'YAML'}
					Disallow_hostPath {tool 'kyverno', severity 'medium', name 'disallow-host-path', fields 'spec_volumes', kinds 'Pod,Volume', doc 'HostPath volumes let Pods use host directories and volumes in containers Using host resources can be used to access shared data or escalate privileges and should not be allowed This policy ensures no hostPath volumes are in use', raw_source 'YAML'}
					Disallow_hostPorts_Range_Alternate {tool 'kyverno', severity 'medium', name 'disallow-host-ports-range', kinds 'Pod', doc 'Access to host ports allows potential snooping of network traffic and should not be allowed by requiring host ports be undefined (recommended) or at minimum restricted to a known list This policy ensures the hostPort field, if defined, is set to either a port in the specified range or to a value of zero This policy is mutually exclusive of the disallow-host-ports policy Note that Kubernetes Pod Security Admission does not support the host port range rule', raw_source 'YAML'}
					Disallow_hostPorts {tool 'kyverno', severity 'medium', name 'disallow-host-ports', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Access to host ports allows potential snooping of network traffic and should not be allowed, or at minimum restricted to a known list This policy ensures the hostPort field is unset or set to 0 ', raw_source 'YAML'}
					Disallow_hostProcess {tool 'kyverno', severity 'medium', name 'disallow-host-process', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Windows pods offer the ability to run HostProcess containers which enables privileged access to the Windows node Privileged access to the host is disallowed in the baseline policy HostProcess pods are an alpha feature as of Kubernetes v122 This policy ensures the hostProcess field, if present, is set to false', raw_source 'YAML'}
					Disallow_Privileged_Containers {tool 'kyverno', severity 'medium', name 'disallow-privileged-containers', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Privileged mode disables most security mechanisms and must not be allowed This policy ensures Pods do not call for privileged mode', raw_source 'YAML'}
					Disallow_procMount {tool 'kyverno', severity 'medium', name 'disallow-proc-mount', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'The default /proc masks are set up to reduce attack surface and should be required This policy ensures nothing but the default procMount can be specified Note that in order for users to deviate from the Default procMount requires setting a feature gate at the API server', raw_source 'YAML'}
					Disallow_SELinux {tool 'kyverno', severity 'medium', name 'disallow-selinux', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers, spec_securityContext, spec_securityContext_seLinuxOptions, spec_securityContext_seLinuxOptions_Xrole, spec_securityContext_seLinuxOptions_Xuser, spec_securityContext_seLinuxOptions_type', kinds 'Pod', doc 'SELinux options can be used to escalate privileges and should not be allowed This policy ensures that the seLinuxOptions field is undefined', raw_source 'YAML'}
					Restrict_AppArmor {tool 'kyverno', severity 'medium', name 'restrict-apparmor-profiles', fields 'metadata_annotations, metadata_annotations_container_apparmor_security_beta_kubernetes_io_*', kinds 'Pod, Annotation', doc 'On supported hosts, the _runtime/default_ AppArmor profile is applied by default The default policy should prevent overriding or disabling the policy, or restrict overrides to an allowed set of profiles This policy ensures Pods do not specify any other AppArmor profiles than runtime/default or localhost/*', raw_source 'YAML'}
					Restrict_Seccomp {tool 'kyverno', severity 'medium', name 'restrict-seccomp', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers, spec_securityContext, spec_securityContext_seccompProfile, spec_securityContext_seccompProfile_type', kinds 'Pod', doc 'The seccomp profile must not be explicitly set to Unconfined This policy,  requiring Kubernetes v119 or later, ensures that seccomp is unset or  set to RuntimeDefault or Localhost', raw_source 'YAML'}
					Restrict_sysctls {tool 'kyverno', severity 'medium', name 'restrict-sysctls', fields 'spec_securityContext, spec_securityContext_sysctls', kinds 'Pod', doc 'Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed safe subset A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node This policy ensures that only those safe subsets can be specified in a Pod', raw_source 'YAML'}
			Pod_Security_Standards_Restricted
				optional
					Disallow_Privilege_Escalation {tool 'kyverno', severity 'medium', name 'disallow-privilege-escalation', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers', kinds 'Pod', doc 'Privilege escalation, such as via set-user-ID or set-group-ID file mode, should not be allowed This policy ensures the allowPrivilegeEscalation field is set to false', raw_source 'YAML'}
					Require_Run_As_Non_Root_User {tool 'kyverno', severity 'medium', name 'require-run-as-non-root-user', fields 'spec_containers, spec_ephemeralContainers, spec_initContainers, spec_securityContext, spec_securityContext_runAsUser', kinds 'Pod', doc 'Containers must be required to run as non-root users This policy ensures runAsUser is either unset or set to a number greater than zero', raw_source 'YAML'}
					Restrict_Volume_Types {tool 'kyverno', severity 'medium', name 'restrict-volume-types', kinds 'Pod,Volume', doc 'In addition to restricting HostPath volumes, the restricted pod security profile limits usage of non-core volume types to those defined through PersistentVolumes This policy blocks any other type of volume other than those in the allow list', raw_source 'YAML'}
			Pod_Security_Admission_EKS_Best_Practices
				optional
					Add_PSA_Namespace_Reporting {tool 'kyverno', severity 'medium', name 'add-psa-namespace-reporting', fields 'metadata_labels, metadata_labels_pod_security_kubernetes_io_*', kinds 'Namespace', doc 'This policy is valuable as it ensures that all namespaces within a Kubernetes  cluster are labeled with Pod Security Admission (PSA) labels, which are crucial for defining security levels and ensuring that pods within a namespace operate  under the defined Pod Security Standard (PSS) By enforcing namespace labeling, This policy audits namespaces to verify the presence of PSA labels  If a namespace is found without the required labels, it generates and maintain  and ClusterPolicy Report in default namespace  This helps administrators identify namespaces that do not comply with the  organization_s security practices and take appropriate action to rectify the  situation', raw_source 'YAML'}
			Pod_Security_Admission
				optional
					Deny_Privileged_Profile {tool 'kyverno', severity 'medium', name 'deny-privileged-profile', kinds 'Namespace', doc 'When Pod Security Admission (PSA) is enforced at the cluster level via an AdmissionConfiguration file which defines a default level at baseline or restricted, setting of a label at the privileged profile will effectively cause unrestricted workloads in that Namespace, overriding the cluster default This may effectively represent a circumvention attempt and should be closely controlled This policy ensures that only those holding the cluster-admin ClusterRole may create Namespaces which assign the label pod-securitykubernetesio/enforce=privileged', raw_source 'YAML'}
			Tekton
				optional
					Require_Tekton_Bundle {tool 'kyverno', severity 'medium', name 'require-tekton-bundle', fields 'spec_pipelineRef, spec_pipelineRef_bundle, spec_taskRef, spec_taskRef_bundle', kinds 'TaskRun, PipelineRun', doc 'PipelineRun and TaskRun resources must be executed from a bundle', raw_source 'YAML'}
					Require_securityContext_for_Tekton_TaskRun {tool 'kyverno', severity 'medium', name 'require-tekton-securitycontext', fields 'status_taskSpec, status_taskSpec_steps', kinds 'TaskRun', doc 'A securityContext is required for each TaskRun step', raw_source 'YAML'}
			Traefik
				optional
					Disallow_Default_TLSOptions {tool 'kyverno', severity 'medium', name 'disallow-default-tlsoptions', kinds 'TLSOption', doc 'The TLSOption CustomResource sets cluster-wide TLS configuration options for Traefik when  none are specified in a TLS router Since this can take effect for all Ingress resources, creating the default TLSOption is a restricted operation This policy ensures that only a cluster-admin can create the default TLSOption resource', raw_source 'YAML'}
			Windows_Security
				optional
					Require_Run_As_ContainerUser_Windows {tool 'kyverno', severity 'medium', name 'require-run-as-containeruser', fields 'spec_containers, spec_initContainers, spec_securityContext, spec_securityContext_windowsOptions, spec_securityContext_windowsOptions_runAsUserName', kinds 'Pod', doc 'Containers must be required to run as ContainerUser This policy ensures that the fields  specsecurityContextwindowsOptionsrunAsUserName, speccontainers[*]securityContextwindowsOptionsrunAsUserName,  specinitContainers[*]securityContextwindowsOptionsrunAsUserName, and  is either unset or set to ContainerUser', raw_source 'YAML'}
			OPAConstraints {abstract}
				optional
					no_shared_ipc_namespace {tool 'trivy', severity 'high', name 'no-shared-ipc-namespace', fields 'spec_template_spec_hostIPC', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Sharing the host_s IPC namespace allows container processes to communicate with processes on the host', RecommendedAction 'Do not set _spectemplatespechostIPC_ to true', raw_source 'OPA-Rego'}
					no_host_network {tool 'trivy', severity 'high', name 'no-host-network', fields 'spec_template_spec_hostNetwork', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Sharing the host_s network namespace permits processes in the pod to communicate with processes bound to the host_s loopback adapter', RecommendedAction 'Do not set _spectemplatespechostNetwork_ to true', raw_source 'OPA-Rego'}
					no_host_pid {tool 'trivy', severity 'high', name 'no-host-pid', fields 'spec_template_spec_hostPID', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Sharing the host_s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration', RecommendedAction 'Do not set _spectemplatespechostPID_ to true', raw_source 'OPA-Rego'}
					no_docker_sock_mount {tool 'trivy', severity 'high', name 'no-docker-sock-mount', fields 'volumes_hostPath_path', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Mounting dockersock from the host can give the container full root access to the host', RecommendedAction 'Do not specify /var/run/dockersocket in _spectemplatevolumeshostPathpath_', raw_source 'OPA-Rego'}
					no_privileged_containers {tool 'trivy', severity 'high', name 'no-privileged-containers', fields 'securityContext_privileged', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Privileged containers share namespaces with the host system and do not offer any security They should be used exclusively for system containers that require high privileges', RecommendedAction 'Change _containers[]securityContextprivileged_ to _false_', raw_source 'OPA-Rego'}
					no_auto_mount_service_token {tool 'trivy', severity 'medium', name 'no-auto-mount-service-token', fields 'spec_automountServiceAccountToken', doc 'ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false', RecommendedAction 'Disable the mounting of service account secret token by setting automountServiceAccountToken to false', raw_source 'OPA-Rego'}
					no_root {tool 'trivy', severity 'medium', name 'no-root', fields 'securityContext_runAsNonRoot', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Force the running image to run as a non-root user to ensure least privileges', RecommendedAction 'Set _containers[]securityContextrunAsNonRoot_ to true', raw_source 'OPA-Rego'}
					use_high_gid {tool 'trivy', severity 'low', name 'use-high-gid', fields 'securityContext_runAsGroup', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Force the container to run with group ID > 10000 to avoid conflicts with the host_s user table', RecommendedAction 'Set _containers[]securityContextrunAsGroup_ to an integer > 10000', raw_source 'OPA-Rego'}
					use_high_uid {tool 'trivy', severity 'low', name 'use-high-uid', fields 'securityContext_runAsUser', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'Force the container to run with user ID > 10000 to avoid conflicts with the host_s user table', RecommendedAction 'Set _containers[]securityContextrunAsUser_ to an integer > 10000', raw_source 'OPA-Rego'}
					no_sysadmin_capability {tool 'trivy', severity 'high', name 'no-sysadmin-capability', fields 'container_securityContext_capabilities_add', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root', RecommendedAction 'Remove the SYS_ADMIN capability from _containers[]securityContextcapabilitiesadd_', raw_source 'OPA-Rego'}
					no_sysmodule_capability {tool 'trivy', severity 'high', name 'no-sysmodule-capability', fields 'container_securityContext_capabilities_add', kinds 'cronjob, daemonset, deployment, deploymentconfig, job, pod, replicaset, replicationcontroller, statefulset', doc 'The SYS_MODULE capability grants attackers the ability to install and remove kernel modules, posing serious security risks', RecommendedAction 'To mitigate potential security risks, it is strongly recommended to remove the SYS_MODULE capability from _containers[]securityContextcapabilitiesadd_ It is advisable to follow the practice of dropping all capabilities and only adding the necessary ones', raw_source 'OPA-Rego'}
			PolarisConstraints {abstract}
				optional
					cpuLimitsMissing {tool 'Polaris', severity 'warning', name_field 'cpuLimitsMissing', kinds 'Container', doc 'CPU limits should be set', category 'Efficiency', raw_source 'YAML with dinamic JSON'}
					cpuRequestsMissing {tool 'Polaris', severity 'warning', name_field 'cpuRequestsMissing', kinds 'Container', doc 'CPU requests should be set', category 'Efficiency', raw_source 'YAML with dinamic JSON'}
					memoryLimitsMissing {tool 'Polaris', severity 'warning', name_field 'memoryLimitsMissing', kinds 'Container', doc 'Memory limits should be set', category 'Efficiency', raw_source 'YAML with dinamic JSON'}
					memoryRequestsMissing {tool 'Polaris', severity 'warning', name_field 'memoryRequestsMissing', kinds 'Container', doc 'Memory requests should be set', category 'Efficiency', raw_source 'YAML with dinamic JSON'}
					deploymentMissingReplicas {tool 'Polaris', severity 'warning', name_field 'deploymentMissingReplicas', kinds 'Controller', doc 'Only one replica is scheduled', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					hpaMaxAvailability {tool 'Polaris', severity 'warning', name_field 'hpaMaxAvailability', kinds 'autoscaling/HorizontalPodAutoscaler', doc 'HPA maxReplicas and minReplicas should be different', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					hpaMinAvailability {tool 'Polaris', severity 'warning', name_field 'hpaMinAvailability', kinds 'autoscaling/HorizontalPodAutoscaler', doc 'HPA minReplicas should be 2 or more', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					livenessProbeMissing {tool 'Polaris', severity 'warning', name_field 'livenessProbeMissing', kinds 'Container', doc 'Liveness probe should be configured', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					metadataAndInstanceMismatched {tool 'Polaris', severity 'warning', name_field 'metadataAndInstanceMismatched', kinds 'Controller', doc 'Label appkubernetesio/instance must match metadataname', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					priorityClassNotSet {tool 'Polaris', severity 'warning', name_field 'priorityClassNotSet', kinds 'PodSpec', doc 'Priority class should be set', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					pullPolicyNotAlways {tool 'Polaris', severity 'warning', name_field 'pullPolicyNotAlways', kinds 'Container', doc 'Image pull policy should be Always', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					readinessProbeMissing {tool 'Polaris', severity 'warning', name_field 'readinessProbeMissing', kinds 'Container', doc 'Readiness probe should be configured', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					tagNotSpecified {tool 'Polaris', severity 'danger', name_field 'tagNotSpecified', kinds 'Container', doc 'Image tag should be specified', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					topologySpreadConstraint {tool 'Polaris', severity 'warning', name_field 'topologySpreadConstraint', kinds 'PodSpec', doc 'Pod should be configured with a valid topology spread constraint', category 'Reliability', raw_source 'YAML with dinamic JSON'}
					automountServiceAccountToken {tool 'Polaris', severity 'warning', name_field 'automountServiceAccountToken', kinds 'PodSpec', doc 'The ServiceAccount will be automounted', category 'Security', raw_source 'YAML with dinamic JSON'}
					dangerousCapabilities {tool 'Polaris', severity 'danger', name_field 'dangerousCapabilities', kinds 'Container', doc 'Container should not have dangerous capabilities', category 'Security', raw_source 'YAML with dinamic JSON'}
					hostIPCSet {tool 'Polaris', severity 'danger', name_field 'hostIPCSet', kinds 'PodSpec', doc 'Host IPC should not be configured', category 'Security', raw_source 'YAML with dinamic JSON'}
					hostNetworkSet {tool 'Polaris', severity 'danger', name_field 'hostNetworkSet', kinds 'PodSpec', doc 'Host network should not be configured', category 'Security', raw_source 'YAML with dinamic JSON'}
					hostPIDSet {tool 'Polaris', severity 'danger', name_field 'hostPIDSet', kinds 'PodSpec', doc 'Host PID should not be configured', category 'Security', raw_source 'YAML with dinamic JSON'}
					hostPortSet {tool 'Polaris', severity 'warning', name_field 'hostPortSet', kinds 'Container', doc 'Host port should not be configured', category 'Security', raw_source 'YAML with dinamic JSON'}
					insecureCapabilities {tool 'Polaris', severity 'warning', name_field 'insecureCapabilities', kinds 'Container', doc 'Container should not have insecure capabilities', category 'Security', raw_source 'YAML with dinamic JSON'}
					linuxHardening {tool 'Polaris', severity 'danger', name_field 'linuxHardening', kinds 'Container', doc 'Use one of AppArmor, Seccomp, SELinux, or dropping Linux Capabilities to restrict containers using unwanted privileges', category 'Security', raw_source 'YAML with dinamic JSON'}
					notReadOnlyRootFilesystem {tool 'Polaris', severity 'warning', name_field 'notReadOnlyRootFilesystem', kinds 'PodSpec', doc 'Filesystem should be read only', category 'Security', raw_source 'YAML with dinamic JSON'}
					privilegeEscalationAllowed {tool 'Polaris', severity 'danger', name_field 'privilegeEscalationAllowed', kinds 'PodSpec', doc 'Privilege escalation should not be allowed', category 'Security', raw_source 'YAML with dinamic JSON'}
					procMount {tool 'Polaris', severity 'warning', name_field 'procMount', kinds 'PodSpec', doc 'Proc mount must not be changed from the default', category 'Security', raw_source 'YAML with dinamic JSON'}
					runAsPrivileged {tool 'Polaris', severity 'danger', name_field 'runAsPrivileged', kinds 'PodSpec', doc 'Should not be running as privileged', category 'Security', raw_source 'YAML with dinamic JSON'}
					runAsRootAllowed {tool 'Polaris', severity 'danger', name_field 'runAsRootAllowed', kinds 'PodSpec', doc 'Should not be allowed to run as root', category 'Security', raw_source 'YAML with dinamic JSON'}
					tlsSettingsMissing {tool 'Polaris', severity 'warning', name_field 'tlsSettingsMissing', kinds 'networking_k8s_io/Ingress', doc 'Ingress does not have TLS configured', category 'Security', raw_source 'YAML with dinamic JSON'}
			GatekeeeperConstraints {abstract}
				optional
					k8spsphostfilesystem {tool 'Gatekeeper', severity 'undefined', category 'Host_Filesystem', kinds 'Pod', doc 'Controls usage of the host filesystem Corresponds to the allowedHostPaths field in a PodSecurityPolicy For more information, see https:_kubernetesio/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems'}
					k8spsphostnamespace {tool 'Gatekeeper', severity 'undefined', category 'Host_Namespace', kinds 'Pod', doc 'Disallows sharing of host PID and IPC namespaces by pod containers Corresponds to the hostPID and hostIPC fields in a PodSecurityPolicy For more information, see https:_kubernetesio/docs/concepts/policy/pod-security-policy/#host-namespaces'}
					k8spsphostnetworkingports {tool 'Gatekeeper', severity 'undefined', category 'Host_Networking_Ports', kinds 'Pod', doc 'Controls usage of host network namespace by pod containers HostNetwork verification happens without exception for exemptImages Specific ports must be specified Corresponds to the hostNetwork and hostPorts fields in a PodSecurityPolicy For more information, see https:_kubernetesio/docs/concepts/policy/pod-security-policy/#host-namespaces'}
					k8spspprivilegedcontainer {tool 'Gatekeeper', severity 'undefined', category 'Privileged_Container', kinds 'Pod', doc 'Controls the ability of any container to enable privileged mode Corresponds to the privileged field in a PodSecurityPolicy For more information, see https:_kubernetesio/docs/concepts/policy/pod-security-policy/#privileged'}
					k8spspreadonlyrootfilesystem {tool 'Gatekeeper', severity 'undefined', category 'Read_Only_Root_Filesystem', kinds 'Pod', doc 'Requires the use of a read-only root file system by pod containers Corresponds to the readOnlyRootFilesystem field in a PodSecurityPolicy For more information, see https:_kubernetesio/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems'}
					k8spspvolumetypes {tool 'Gatekeeper', severity 'undefined', category 'Volume_Types', kinds 'Pod', doc 'Restricts mountable volume types to those specified by the user Corresponds to the volumes field in a PodSecurityPolicy For more information, see https:_kubernetesio/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems'}
			Pod.PodFeatures
			ServAcc.ServiceAccountFeatures
			RoleBinding.RoleBindingFeatures
			ClusRole.ClusterRoleBindingFeatures
			Serv.ServiceFeatures
			Ingress.IngressFeatures
			Job.JobFeatures
			DaemonSet.DaemonSetFeatures
			Deployment.DeploymentFeatures
			StatefulSet.StatefulSetFeatures
			Secret.SecretFeatures
			PersistVolumeClaim.PersistentVolumeClaimFeatures
			PodDisrupBud.PodDisruptionBudgetFeatures
			CronJob.CronJobFeatures
			ReplicaSet.ReplicaSetFeatures
			RepController.ReplicationControllerFeatures
			Container.ContainerFeatures
			PodList.PodListFeatures
			PodTemplate.PodTemplateFeatures
			PodTemplateList.PodTemplateListFeatures
			PodTemplateSpec.PodTemplateSpecFeatures
			HorizontalPodAutoscaler.HorizontalPodAutoscalerFeatures
constraints
	Require_aws_node_DaemonSet_use_IRSA => DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_serviceAccountName != aws-node
	Require_Encryption_with_AWS_LoadBalancers => (Serv.io_k8s_api_core_v1_Service_metadata_annotations_KeyMap == 'service_beta_kubernetes_io/aws-load-balancer-ssl-cert' & Serv.io_k8s_api_core_v1_Service_metadata_annotations_ValueMap == '?*') | (Serv.io_k8s_api_core_v1_Service_spec_type_LoadBalancer == true)
	Check_deprecated_APIs => ()
	Disallow_CRI_socket_mounts => (Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath_path != /var/run/docker_sock & Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath_path != /var/run/containerd/containerd_sock & Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath_path != /var/run/crio/crio_sock & Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath_path != /var/run/cri-dockerd_sock)
	Disallow_Default_Namespace => (Pod.io_k8s_api_core_v1_Pod_metadata_namespace != 'default' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_metadata_namespace != 'default' & Deployment.io_k8s_api_apps_v1_Deployment_metadata_namespace != 'default' & Job.io_k8s_api_batch_v1_Job_metadata_namespace != 'default' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_metadata_namespace != 'default')
	Disallow_empty_Ingress_host => ()
	Require_Labels => Pod.io_k8s_api_core_v1_Pod_metadata_labels == '{'app.kubernetes.io/name': '?*'}'
	Require_Limits_and_Requests => (Pod.io_k8s_api_core_v1_Pod_spec_containers_resources_requests_memory == ?* & Pod.io_k8s_api_core_v1_Pod_spec_containers_resources_requests_cpu == ?* & Pod.io_k8s_api_core_v1_Pod_spec_containers_resources_limits_memory == ?* & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_resources_requests_memory == ?* & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_resources_requests_cpu == ?* & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_resources_limits_memory == ?* & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_resources_requests_memory == ?* & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_resources_requests_cpu == ?* & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_resources_limits_memory == ?*)
	Require_Read_Only_Root_Filesystem => Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_readOnlyRootFilesystem
	Restrict_Image_Registries => (Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_image == eu_foo_io/* | bar_io/* & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_image == eu_foo_io/* | bar_io/* & Pod.io_k8s_api_core_v1_Pod_spec_containers_image == eu_foo_io/* | bar_io/*)
	Disallow_NodePort => !Serv.io_k8s_api_core_v1_Service_spec_type_NodePort
	Restrict_External_IPs => !Serv.io_k8s_api_core_v1_Service_spec_externalIPs
	Certificate_max_duration_100_days => ()
	Restrict_issuer => (Kubernetes.io_k8s_api_core_v1_Certificate_spec_dnsNames == ['*.corp.com'] & Kubernetes.io_k8s_api_core_v1_Certificate_spec_issuerRef_name == our-corp-issuer & Kubernetes.io_k8s_api_core_v1_Certificate_spec_issuerRef_kind == ClusterIssuer & Kubernetes.io_k8s_api_core_v1_Certificate_spec_issuerRef_group == cert-manager_io)
	Enforce_Consul_min_TLS_version => Kubernetes.io_k8s_api_core_v1_Mesh_spec_tls_incoming_tlsMinVersion == TLSv1_2
	Verify_Flux_Sources => (Kubernetes.io_k8s_api_core_v1_GitRepository_spec_url == https://github_com/myorg/?* | ssh://git@github_com:myorg/?* & Kubernetes.io_k8s_api_core_v1_Bucket_spec_endpoint == *_myorg_com & Kubernetes.io_k8s_api_core_v1_HelmRepository_spec_url == https://?*_myorg_com/* & Kubernetes.io_k8s_api_core_v1_ImageRepository_spec_image == ghcr_io/myorg/*)
	Verify_Git_Repositories => Kubernetes.io_k8s_api_core_v1_GitRepository_spec_url == https://github_com/fluxcd/?* | ssh://git@github_com:fluxcd/?*
	Enforce_Istio_Ambient_Mode => Kubernetes.io_k8s_api_core_v1_Namespace_metadata_labels == '{'istio.io/dataplane-mode': 'ambient'}'
	Enforce_Istio_Sidecar_Injection => Kubernetes.io_k8s_api_core_v1_Namespace_metadata_labels == '{'istio-injection': 'enabled'}'
	Enforce_Istio_Strict_mTLS => Kubernetes.io_k8s_api_core_v1_PeerAuthentication_spec_mtls_mode == UNSET | STRICT
	Enforce_Istio_TLS_on_Hosts_and_Host_Subnets => Kubernetes.io_k8s_api_core_v1_DestinationRule_spec_trafficPolicy_tls_mode != DISABLE
	Prevent_Disabling_Istio_Sidecar_Injection => (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'sidecar_istio_io/inject' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap != 'false')
	Check_Kasten_3_2_1_Backup_Policy => ()
	Check_Data_Protection_By_Label => (Deployment.io_k8s_api_apps_v1_Deployment_metadata_labels == '{'dataprotection': 'kasten-example|none'}' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_metadata_labels == '{'dataprotection': 'kasten-example|none'}')
	Check_Kasten_Location_Profile_is_Immutable => ()
	Validate_Data_Protection_with_Kasten_Preset_Label => Kubernetes.io_k8s_api_core_v1_Namespace_metadata_labels == '{'dataprotection': 'gold|silver|bronze|none'}'
	Require_Kubecost_Labels => Pod.io_k8s_api_core_v1_Pod_metadata_labels == '{'owner': '?*', 'team': '?*', 'department': '?*', 'app': '?*', 'env': '?*'}'
	Enforce_instanceTypes => (Kubernetes.io_k8s_api_core_v1_VirtualMachine_spec_instancetype_name == ?* & Kubernetes.io_k8s_api_core_v1_VirtualMachine_spec_preference_name == ?*)
	Prevent_Linkerd_Pod_Injection_Override => (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'linkerd_io/inject' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap != 'disabled')
	Prevent_Linkerd_Port_Skipping => (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'X(config_linkerd_io/skip-inbound-ports' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap == 'null') | (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'X(config_linkerd_io/skip-outbound-ports' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap == 'null')
	Require_Linkerd_Mesh_Injection => (Kubernetes.io_k8s_api_core_v1_Namespace_metadata_annotations_KeyMap == 'linkerd_io/inject' & Kubernetes.io_k8s_api_core_v1_Namespace_metadata_annotations_ValueMap == 'enabled')
	Disallow_Custom_Snippets => (Kubernetes.io_k8s_api_core_v1_ConfigMap_data_=allow_snippet_annotations == 'false' & Ingress.io_k8s_api_core_v1_networking_k8s_io_v1_Ingress_metadata_annotations_KeyMap == 'X(*-snippet') | (Ingress.io_k8s_api_core_v1_networking_k8s_io_v1_Ingress_metadata_annotations_ValueMap == '?*')
	Restrict_NGINX_Ingress_path_values => ()
	Disallow_deprecated_APIs => ()
	Allowed_Annotations => ()
	Allowed_Label_Changes => (Kubernetes.io_k8s_api_core_v1_Pod__request_operation_||_'BACKGROUND' == 'UPDATE' & Kubernetes.io_k8s_api_apps_v1_Deployment__request_operation_||_'BACKGROUND' == 'UPDATE' & Kubernetes.io_k8s_api_apps_v1_StatefulSet__request_operation_||_'BACKGROUND' == 'UPDATE' & Kubernetes.io_k8s_api_apps_v1_DaemonSet__request_operation_||_'BACKGROUND' == 'UPDATE' & Job.io_k8s_api_batch_v1_Job__request_operation_||_'BACKGROUND' == 'UPDATE' & Job.io_k8s_api_batch_v1_CronJob__request_operation_||_'BACKGROUND' == 'UPDATE')
	Block_Ephemeral_Containers => !Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers
	Block_kubectl_cp_command_by_Pod_Label => Kubernetes.io_k8s_api_core_v1_Pod_exec__request_operation_||_'BACKGROUND' == 'CONNECT'
	Block_Pod_Exec_by_Pod_and_Container => (Kubernetes.io_k8s_api_core_v1_Pod_exec__request_operation_||_'BACKGROUND' == 'CONNECT' & Kubernetes.io_k8s_api_core_v1_Pod_exec__request_name == 'myapp-maintenance*')
	Check_Environment_Variables => (Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & Pod.io_k8s_api_core_v1_Pod_spec_containers_env_name == DISABLE_OPA & Pod.io_k8s_api_core_v1_Pod_spec_containers_env_value != true)
	Ensure_Valid_Ingress_NGINX_Controller_and_Annotations => ()
	Check_Long_Lived_Secrets_in_ServiceAccounts => ()
	Deny_Commands_in_Exec_Probe => ()
	Deny_Secret_Service_Account_Token_Type => Secret.io_k8s_api_core_v1_Secret_type != kubernetes_io/service-account-token
	Disallow_all_Secrets => (Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_env_valueFrom_secretKeyRef & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_env_valueFrom_secretKeyRef & Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_containers_env_valueFrom_secretKeyRef & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_envFrom_secretRef & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_envFrom_secretRef & Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_containers_envFrom_secretRef & !Pod.io_k8s_api_core_v1_Pod_spec_volumes_secret)
	Disallow_Localhost_ExternalName_Services => (Serv.io_k8s_api_core_v1_Service_spec_type_ExternalName == true & Serv.io_k8s_api_core_v1_Service_spec_externalName != localhost)
	Disallow_Secrets_from_Env_Vars => (Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_containers_env_valueFrom_secretKeyRef & Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & !Pod.io_k8s_api_core_v1_Pod_spec_containers_envFrom_secretRef)
	Docker_Socket_Requires_Label => (Pod.io_k8s_api_core_v1_Pod_metadata_labels == '{'allow-docker': 'true'}' & Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath_path == /var/run/docker_sock)
	Enforce_pod_duration => ()
	Enforce_ReadWriteOncePod => PersistVolumeClaim.io_k8s_api_core_v1_PersistentVolumeClaim_spec_accessModes_StringValue == ReadWriteOncePod
	Validate_Probes => ()
	Forbid_CPU_Limits => (Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & Pod.io_k8s_api_core_v1_Pod_spec_containers_resources_limits_cpu == 'None')
	Require_imagePullPolicy_Always => (Pod.io_k8s_api_core_v1_Pod_spec_containers_image != *:latest | *:* & Pod.io_k8s_api_core_v1_Pod_spec_containers_imagePullPolicy == Always)
	Inspect_CertificateSigningRequest => ()
	Memory_Requests_Equal_Limits => ()
	Namespace_Protection => ()
	PodDisruptionBudget_maxUnavailable_Non_Zero => PodDisrupBud.io_k8s_api_policy_v1_PodDisruptionBudget_spec_maxUnavailable_asInteger > 0
	Prevent_cr8escape_CVE_2022_0811 => Pod.io_k8s_api_core_v1_Pod_spec_securityContext_sysctls_value != *+* & *=*
	Prevent_Duplicate_HorizontalPodAutoscalers => Kubernetes.io_k8s_api_autoscaling_v2_HorizontalPodAutoscaler_spec_scaleTargetRef_kind == Deployment | StatefulSet | ReplicaSet | DaemonSet
	Prevent_Duplicate_VerticalPodAutoscalers => Kubernetes.io_k8s_api_core_v1_VerticalPodAutoscaler_spec_targetRef_kind == Deployment | StatefulSet | ReplicaSet | DaemonSet
	Require_Annotations => (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'corp_org/department' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap == '?*')
	Require_Container_Port_Names => (Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & Pod.io_k8s_api_core_v1_Pod_spec_containers_ports_name == *)
	Require_CPU_Limits => (Pod.io_k8s_api_core_v1_Pod_spec_containers_name == * & Pod.io_k8s_api_core_v1_Pod_spec_containers_resources_limits_cpu == ?* & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_name == * & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_resources_limits_cpu == ?* & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_name == * & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_resources_limits_cpu == ?*)
	Require_Multiple_Replicas => Deployment.io_k8s_api_apps_v1_Deployment_spec_replicas > 1
	Require_Images_Use_Checksums => (Pod.io_k8s_api_core_v1_Pod_spec_containers_image == *@* & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_image == *@* & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_image == *@*)
	Require_Pod_priorityClassName => Pod.io_k8s_api_core_v1_Pod_spec_priorityClassName == ?*
	Require_QoS_Burstable => ()
	Require_Reasonable_PodDisruptionBudgets => ()
	Require_StorageClass => (PersistVolumeClaim.io_k8s_api_core_v1_PersistentVolumeClaim_spec_storageClassName == ?* & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_volumeClaimTemplates_spec_storageClassName == ?*)
	Restrict_Annotations => (Deployment.io_k8s_api_apps_v1_Deployment_metadata_annotations_KeyMap == 'X(fluxcd_io' & Job.io_k8s_api_batch_v1_CronJob_metadata_annotations_KeyMap == 'X(fluxcd_io') | (Job.io_k8s_api_batch_v1_Job_metadata_annotations_KeyMap == 'X(fluxcd_io' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_metadata_annotations_KeyMap == 'X(fluxcd_io') | (DaemonSet.io_k8s_api_apps_v1_DaemonSet_metadata_annotations_KeyMap == 'X(fluxcd_io' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'X(fluxcd_io') | (Deployment.io_k8s_api_apps_v1_Deployment_metadata_annotations_ValueMap == '*?' & Job.io_k8s_api_batch_v1_CronJob_metadata_annotations_ValueMap == '*?') | (Job.io_k8s_api_batch_v1_Job_metadata_annotations_ValueMap == '*?' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_metadata_annotations_ValueMap == '*?') | (DaemonSet.io_k8s_api_apps_v1_DaemonSet_metadata_annotations_ValueMap == '*?' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap == '*?')
	Restrict_Binding_to_Cluster_Admin => (RoleBinding.io_k8s_api_rbac_v1_RoleBinding_roleRef_name != 'cluster-admin' & ClusRole.io_k8s_api_rbac_v1_ClusterRoleBinding_roleRef_name != 'cluster-admin')
	Restrict_Binding_System_Groups => (RoleBinding.io_k8s_api_rbac_v1_RoleBinding_subjects_name != 'system:anonymous' & ClusRole.io_k8s_api_rbac_v1_ClusterRoleBinding_subjects_name != 'system:anonymous' & RoleBinding.io_k8s_api_rbac_v1_RoleBinding_subjects_name != 'system:unauthenticated' & ClusRole.io_k8s_api_rbac_v1_ClusterRoleBinding_subjects_name != 'system:unauthenticated' & RoleBinding.io_k8s_api_rbac_v1_RoleBinding_subjects_name != 'system:masters' & ClusRole.io_k8s_api_rbac_v1_ClusterRoleBinding_subjects_name != 'system:masters')
	Restrict_ClusterRole_with_Nodes_Proxy => ()
	Restrict_control_plane_scheduling => (Pod.io_k8s_api_core_v1_Pod_spec_tolerations_key != node-role_kubernetes_io/master & Pod.io_k8s_api_core_v1_Pod_spec_tolerations_key != node-role_kubernetes_io/control-plane)
	Restrict_Edit_for_Endpoints_CVE_2021_25740 => ()
	Restrict_Ingress_Classes => (Ingress.io_k8s_api_networking_v1_Ingress_metadata_annotations_KeyMap == 'kubernetes_io/ingress_class' & Ingress.io_k8s_api_networking_v1_Ingress_metadata_annotations_ValueMap == 'HAProxy') | (Ingress.io_k8s_api_networking_v1_Ingress_metadata_annotations_KeyMap == 'kubernetes_io/ingress_class' & Ingress.io_k8s_api_networking_v1_Ingress_metadata_annotations_ValueMap == 'nginx')
	Restrict_Ingress_defaultBackend => !Ingress.io_k8s_api_networking_v1_Ingress_spec_defaultBackend
	Restrict_Jobs => Job.io_k8s_api_batch_v1_Job_metadata_ownerReferences_kind == 'CronJob'
	Disallow_Service_Type_LoadBalancer => !Serv.io_k8s_api_core_v1_Service_spec_type_LoadBalancer
	Restrict_Node_Affinity => !Pod.io_k8s_api_core_v1_Pod_spec_affinity_nodeAffinity
	Restrict_node_label_changes => (Kubernetes.io_k8s_api_core_v1_Node__request_operation_||_'BACKGROUND' == 'UPDATE' & Kubernetes.io_k8s_api_core_v1_Node__request_oldObject_metadata_labels_foo == '?*')
	Restrict_Pod_Controller_ServiceAccount_Updates => ()
	Restrict_Auto_Mount_of_Service_Account_Tokens_in_Service_Account => !ServAcc.io_k8s_api_core_v1_ServiceAccount_automountServiceAccountToken
	Restrict_Scale => (Deployment.io_k8s_api_core_v1_Deployment_scale_status_selector == '*type=monitoring*' & Deployment.io_k8s_api_core_v1_Deployment_scale_spec_replicas < 9 & Deployment.io_k8s_api_apps_v1_Deployment_spec_replicas < 5 & Pod.io_k8s_api_core_v1_Deployment/scale_spec_replicas > 8)
	Restrict_Secret_Verbs_in_Roles => ()
	Restrict_Service_Port_Range => (Serv.io_k8s_api_core_v1_Service_spec_ports_port > 31999 & Serv.io_k8s_api_core_v1_Service_spec_ports_port < 33001)
	Restrict_StorageClass => Kubernetes.io_k8s_api_storage_v1_StorageClass_reclaimPolicy == 'Delete'
	Validate_User_ID_Group_ID_and_FS_Group => (Pod.io_k8s_api_core_v1_Pod_spec_securityContext_runAsUser = 1000 & Pod.io_k8s_api_core_v1_Pod_spec_securityContext_runAsGroup = 3000 & Pod.io_k8s_api_core_v1_Pod_spec_securityContext_fsGroup = 2000)
	Spread_Pods_Across_Nodes_&_Zones => ()
	Verify_VerticalPodAutoscaler_Target => (Kubernetes.io_k8s_api_core_v1_VerticalPodAutoscaler_spec_targetRef_kind == Deployment | StatefulSet | ReplicaSet | DaemonSet & Pod.io_k8s_api_core_v1_VerticalPodAutoscaler_spec_targetRef_name = {{ targets }})
	Disallow_Capabilities => ((Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = AUDIT_WRITE | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = CHOWN | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = DAC_OVERRIDE | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = FOWNER | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = FSETID | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = KILL | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = MKNOD | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = NET_BIND_SERVICE | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = SETFCAP | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = SETGID | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = SETPCAP | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = SETUID | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add = SYS_CHROOT) & (Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = AUDIT_WRITE | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = CHOWN | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = DAC_OVERRIDE | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = FOWNER | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = FSETID | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = KILL | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = MKNOD | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = NET_BIND_SERVICE | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = SETFCAP | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = SETGID | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = SETPCAP | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = SETUID | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add = SYS_CHROOT) & (Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = AUDIT_WRITE | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = CHOWN | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = DAC_OVERRIDE | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = FOWNER | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = FSETID | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = KILL | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = MKNOD | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = NET_BIND_SERVICE | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = SETFCAP | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = SETGID | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = SETPCAP | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = SETUID | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add = SYS_CHROOT))
	Disallow_Host_Namespaces => (!Pod.io_k8s_api_core_v1_Pod_spec_hostPID & !Pod.io_k8s_api_core_v1_Pod_spec_hostIPC & !Pod.io_k8s_api_core_v1_Pod_spec_hostNetwork)
	Disallow_hostPath => !Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath
	Disallow_hostPorts_Range_Alternate => (((Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_ports_hostPort > 4999 & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_ports_hostPort < 6001) | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_ports_hostPort == 0 ) & ((Pod.io_k8s_api_core_v1_Pod_spec_initContainers_ports_hostPort > 4999 & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_ports_hostPort < 6001) | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_ports_hostPort == 0 ) & ((Pod.io_k8s_api_core_v1_Pod_spec_containers_ports_hostPort > 4999 & Pod.io_k8s_api_core_v1_Pod_spec_containers_ports_hostPort < 6001) | Pod.io_k8s_api_core_v1_Pod_spec_containers_ports_hostPort == 0 ))
	Disallow_hostPorts => (Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_ports_hostPort == 0  & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_ports_hostPort == 0  & Pod.io_k8s_api_core_v1_Pod_spec_containers_ports_hostPort == 0 )
	Disallow_hostProcess => (!Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_windowsOptions_hostProcess & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_windowsOptions_hostProcess & !Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_windowsOptions_hostProcess)
	Disallow_Privilege_Escalation => (!Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_allowPrivilegeEscalation & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_allowPrivilegeEscalation & !Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_allowPrivilegeEscalation)
	Disallow_Privileged_Containers => (!Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_privileged & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_privileged & !Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_privileged)
	Disallow_procMount => (Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_procMount_nameStr == Default & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_procMount_nameStr == Default & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_procMount_nameStr == Default)
	Disallow_SELinux => (Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seLinuxOptions_type == container_t | container_init_t | container_kvm_t & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seLinuxOptions_type == container_t | container_init_t | container_kvm_t & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seLinuxOptions_type == container_t | container_init_t | container_kvm_t & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seLinuxOptions_type == container_t | container_init_t | container_kvm_t & !Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seLinuxOptions_user & !Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seLinuxOptions_role & !Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seLinuxOptions_user & !Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seLinuxOptions_role & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seLinuxOptions_user & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seLinuxOptions_role & !Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seLinuxOptions_user & !Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seLinuxOptions_role)
	Require_Run_As_Non_Root_User => (Pod.io_k8s_api_core_v1_Pod_spec_securityContext_runAsUser > 0 & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_runAsUser > 0 & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_runAsUser > 0 & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_runAsUser > 0)
	Restrict_AppArmor => (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'container_apparmor_security_beta_kubernetes_io' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap == 'runtime/default') | (Pod.io_k8s_api_core_v1_Pod_metadata_annotations_KeyMap == 'container_apparmor_security_beta_kubernetes_io' & Pod.io_k8s_api_core_v1_Pod_metadata_annotations_ValueMap == 'localhost')
	Restrict_Seccomp => ((!Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seccompProfile_type | (Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seccompProfile_type => Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seccompProfile_type_RuntimeDefault | Pod.io_k8s_api_core_v1_Pod_spec_securityContext_seccompProfile_type_Localhost)) & (!Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seccompProfile_type | (Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seccompProfile_type => Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seccompProfile_type_RuntimeDefault | Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_seccompProfile_type_Localhost)) & (!Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seccompProfile_type | (Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seccompProfile_type => Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seccompProfile_type_RuntimeDefault | Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_seccompProfile_type_Localhost)) & (!Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seccompProfile_type | (Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seccompProfile_type => Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seccompProfile_type_RuntimeDefault | Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_seccompProfile_type_Localhost)))
	Restrict_sysctls => Pod.io_k8s_api_core_v1_Pod_spec_securityContext_sysctls_name == kernel_shm_rmid_forced | net_ipv4_ip_local_port_range | net_ipv4_ip_unprivileged_port_start | net_ipv4_tcp_syncookies | net_ipv4_ping_group_range
	Restrict_Volume_Types => (Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = name | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = configMap | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = csi | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = downwardAPI | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = emptyDir | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = ephemeral | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = persistentVolumeClaim | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = projected | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = secret | Pod.io_k8s_api_core_v1_Pod_spec_volumes_keys@_||_'' = )
	Add_PSA_Namespace_Reporting => Kubernetes.io_k8s_api_core_v1_Namespace_metadata_labels == '{'pod-security.kubernetes.io/*': '?*'}'
	Deny_Privileged_Profile => ()
	Require_Tekton_Bundle => (Kubernetes.io_k8s_api_core_v1_PipelineRun_spec_pipelineRef_bundle == ?* & Kubernetes.io_k8s_api_core_v1_TaskRun_spec_taskRef_bundle == ?*)
	Require_securityContext_for_Tekton_TaskRun => Kubernetes.io_k8s_api_core_v1_tekton_dev_v1beta1_TaskRun_status_status_=taskSpec == '{'steps': [{'(name)': '!digest-to-results', 'securityContext': {'privileged': False, 'allowPrivilegeEscalation': False}}]}'
	Disallow_Default_TLSOptions => ()
	Require_Run_As_ContainerUser_Windows => (Pod.io_k8s_api_core_v1_Pod_spec_securityContext_windowsOptions_runAsUserName == ContainerUser & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_windowsOptions_runAsUserName == ContainerUser & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_windowsOptions_runAsUserName == ContainerUser)
	no_shared_ipc_namespace => !CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_hostIPC & !DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_hostIPC & !Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_hostIPC & !Job.io_k8s_api_batch_v1_Job_spec_template_spec_hostIPC & !ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_hostIPC & !RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_hostIPC & !StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_hostIPC
	no_host_network => !CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_hostNetwork & !DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_hostNetwork & !Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_hostNetwork & !Job.io_k8s_api_batch_v1_Job_spec_template_spec_hostNetwork & !ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_hostNetwork & !RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_hostNetwork & !StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_hostNetwork
	no_host_pid => !CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_hostPID & !DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_hostPID & !Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_hostPID & !Job.io_k8s_api_batch_v1_Job_spec_template_spec_hostPID & !ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_hostPID & !RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_hostPID & !StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_hostPID
	no_docker_sock_mount => CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock' & Pod.io_k8s_api_core_v1_Pod_spec_volumes_hostPath_path != '/var/run/docker_sock' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_volumes_hostPath_path != '/var/run/docker_sock'
	no_privileged_containers => !CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_containers_securityContext_privileged & !CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_initContainers_securityContext_privileged & !CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_ephemeralContainers_securityContext_privileged & !DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_containers_securityContext_privileged & !DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_initContainers_securityContext_privileged & !DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_ephemeralContainers_securityContext_privileged & !Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_containers_securityContext_privileged & !Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_initContainers_securityContext_privileged & !Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_ephemeralContainers_securityContext_privileged & !Job.io_k8s_api_batch_v1_Job_spec_template_spec_containers_securityContext_privileged & !Job.io_k8s_api_batch_v1_Job_spec_template_spec_initContainers_securityContext_privileged & !Job.io_k8s_api_batch_v1_Job_spec_template_spec_ephemeralContainers_securityContext_privileged & !Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_privileged & !Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_privileged & !Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_privileged & !ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_containers_securityContext_privileged & !ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_initContainers_securityContext_privileged & !ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_ephemeralContainers_securityContext_privileged & !RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_containers_securityContext_privileged & !RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_initContainers_securityContext_privileged & !RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_ephemeralContainers_securityContext_privileged & !StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_containers_securityContext_privileged & !StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_initContainers_securityContext_privileged & !StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_ephemeralContainers_securityContext_privileged
	no_auto_mount_service_token => !Pod.io_k8s_api_core_v1_Pod_spec_automountServiceAccountToken & !PodList.io_k8s_api_core_v1_PodList_items_spec_automountServiceAccountToken & !PodTemplate.io_k8s_api_core_v1_PodTemplate_template_spec_automountServiceAccountToken & !PodTemplateList.io_k8s_api_core_v1_PodTemplateList_items_template_spec_automountServiceAccountToken & !PodTemplateSpec.io_k8s_api_core_v1_PodTemplateSpec_spec_automountServiceAccountToken
	no_root => CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_containers_securityContext_runAsNonRoot & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_initContainers_securityContext_runAsNonRoot & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_containers_securityContext_runAsNonRoot & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_initContainers_securityContext_runAsNonRoot & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_containers_securityContext_runAsNonRoot & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_initContainers_securityContext_runAsNonRoot & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot & Job.io_k8s_api_batch_v1_Job_spec_template_spec_containers_securityContext_runAsNonRoot & Job.io_k8s_api_batch_v1_Job_spec_template_spec_initContainers_securityContext_runAsNonRoot & Job.io_k8s_api_batch_v1_Job_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_runAsNonRoot & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_runAsNonRoot & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_runAsNonRoot & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_containers_securityContext_runAsNonRoot & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_initContainers_securityContext_runAsNonRoot & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_containers_securityContext_runAsNonRoot & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_initContainers_securityContext_runAsNonRoot & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_containers_securityContext_runAsNonRoot & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_initContainers_securityContext_runAsNonRoot & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_ephemeralContainers_securityContext_runAsNonRoot
	use_high_gid => CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & Job.io_k8s_api_batch_v1_Job_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & Job.io_k8s_api_batch_v1_Job_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & Job.io_k8s_api_batch_v1_Job_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_runAsGroup_valueInt > 10000 & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000 & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_containers_securityContext_runAsGroup_valueInt > 10000 & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_initContainers_securityContext_runAsGroup_valueInt > 10000 & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_ephemeralContainers_securityContext_runAsGroup_valueInt > 10000
	use_high_uid => CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & Job.io_k8s_api_batch_v1_Job_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & Job.io_k8s_api_batch_v1_Job_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & Job.io_k8s_api_batch_v1_Job_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_runAsUser_valueInt > 10000 & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000 & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_containers_securityContext_runAsUser_valueInt > 10000 & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_initContainers_securityContext_runAsUser_valueInt > 10000 & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_ephemeralContainers_securityContext_runAsUser_valueInt > 10000
	no_sysadmin_capability => CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_ADMIN'
	no_sysmodule_capability => CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & CronJob.io_k8s_api_batch_v1_CronJob_spec_jobTemplate_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Deployment.io_k8s_api_apps_v1_Deployment_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Job.io_k8s_api_batch_v1_Job_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Pod.io_k8s_api_core_v1_Pod_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & Pod.io_k8s_api_core_v1_Pod_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & RepController.io_k8s_api_core_v1_ReplicationController_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_containers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_initContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_spec_template_spec_ephemeralContainers_securityContext_capabilities_add_StringValue != 'SYS_MODULE'
	cpuLimitsMissing => Container.io_k8s_api_core_v1_Container_resources_limits
	cpuRequestsMissing => Container.io_k8s_api_core_v1_Container_resources_requests
	hpaMaxAvailability => HorizontalPodAutoscaler.io_k8s_api_autoscaling_v1_HorizontalPodAutoscaler_spec_maxReplicas > HorizontalPodAutoscaler.io_k8s_api_autoscaling_v1_HorizontalPodAutoscaler_spec_minReplicas & HorizontalPodAutoscaler.io_k8s_api_autoscaling_v1_HorizontalPodAutoscaler_spec_maxReplicas > 1
	hpaMinAvailability => HorizontalPodAutoscaler.io_k8s_api_autoscaling_v1_HorizontalPodAutoscaler_spec_minReplicas > 2
	livenessProbeMissing => Container.io_k8s_api_core_v1_Container_livenessProbe
	metadataAndInstanceMismatched => (Deployment.io_k8s_api_apps_v1_Deployment_metadata_labels_KeyMap == 'app_kubernetes_io/instance' & Deployment.io_k8s_api_apps_v1_Deployment_metadata_labels_ValueMap == '_metadata_name') | (StatefulSet.io_k8s_api_apps_v1_StatefulSet_metadata_labels_KeyMap == 'app_kubernetes_io/instance' & StatefulSet.io_k8s_api_apps_v1_StatefulSet_metadata_labels_ValueMap == '_metadata_name') | (DaemonSet.io_k8s_api_apps_v1_DaemonSet_metadata_labels_KeyMap == 'app_kubernetes_io/instance' & DaemonSet.io_k8s_api_apps_v1_DaemonSet_metadata_labels_ValueMap == '_metadata_name') | (Job.io_k8s_api_batch_v1_Job_metadata_labels_KeyMap == 'app_kubernetes_io/instance' & Job.io_k8s_api_batch_v1_Job_metadata_labels_ValueMap == '_metadata_name') | (CronJob.io_k8s_api_batch_v1_CronJob_metadata_labels_KeyMap == 'app_kubernetes_io/instance' & CronJob.io_k8s_api_batch_v1_CronJob_metadata_labels_ValueMap == '_metadata_name') | (ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_metadata_labels_KeyMap == 'app_kubernetes_io/instance' & ReplicaSet.io_k8s_api_apps_v1_ReplicaSet_metadata_labels_ValueMap == '_metadata_name')
	pullPolicyNotAlways => (Container.io_k8s_api_core_v1_Container_imagePullPolicy_Always)
	readinessProbeMissing => Container.io_k8s_api_core_v1_Container_readinessProbe
	tagNotSpecified => (Container.io_k8s_api_core_v1_Container_image != '^+:latest$')
	topologySpreadConstraint => Pod.io_k8s_api_core_v1_Pod_spec_topologySpreadConstraints
	dangerousCapabilities => (Container.io_k8s_api_core_v1_Container_securityContext_capabilities_add_StringValue != 'ALL') & (Container.io_k8s_api_core_v1_Container_securityContext_capabilities_add_StringValue != 'SYS_ADMIN') & (Container.io_k8s_api_core_v1_Container_securityContext_capabilities_add_StringValue != 'NET_ADMIN')
	hostPortSet => Container.io_k8s_api_core_v1_Container_ports_hostPort == 0
	insecureCapabilities => (Container.io_k8s_api_core_v1_Container_securityContext_capabilities_drop_StringValue == 'ALL')
	linuxHardening => (Container.io_k8s_api_core_v1_Container_securityContext_seccompProfile_type_Unconfined)
	procMount => (Pod.io_k8s_api_core_v1_Pod_spec_containers_securityContext_procMount_StringValue == 'Default')
	tlsSettingsMissing => Ingress.io_k8s_api_networking_v1_Ingress_spec_tls